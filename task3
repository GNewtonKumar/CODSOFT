import torch
import torchvision.transforms as transforms
from torch.utils.data import DataLoader
from torchvision.datasets import CocoCaptions
from torchvision.models import resnet152
import torch.nn as nn
import torch.optim as optim
import torchvision.models as models
import torch.nn.functional as F

# Define image preprocessing pipeline
transform = transforms.Compose([
    transforms.Resize((224, 224)),
    transforms.ToTensor(),
    transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)),
])

# Load pre-trained ResNet-152 model
resnet = models.resnet152(pretrained=True)
modules = list(resnet.children())[:-1]  # Remove last layer
resnet = nn.Sequential(*modules)
for param in resnet.parameters():
    param.requires_grad = False

# Define RNN-based captioning model
class CaptioningModel(nn.Module):
    def __init__(self, input_size, hidden_size, vocab_size, num_layers=1):
        super(CaptioningModel, self).__init__()
        self.hidden_size = hidden_size
        self.embedding = nn.Embedding(vocab_size, input_size)
        self.rnn = nn.GRU(input_size, hidden_size, num_layers, batch_first=True)
        self.fc = nn.Linear(hidden_size, vocab_size)

    def forward(self, features, captions):
        embeddings = self.embedding(captions)
        embeddings = torch.cat((features.unsqueeze(1), embeddings), dim=1)
        output, _ = self.rnn(embeddings)
        output = self.fc(output)
        return output

# Set device
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

# Define hyperparameters
hidden_size = 512
num_layers = 1
learning_rate = 0.001
num_epochs = 5
vocab_size = len(vocab)

# Initialize model, criterion, and optimizer
model = CaptioningModel(input_size=2048, hidden_size=hidden_size, vocab_size=vocab_size, num_layers=num_layers).to(device)
criterion = nn.CrossEntropyLoss(ignore_index=vocab['<PAD>'])
optimizer = optim.Adam(model.parameters(), lr=learning_rate)

# Train the model
for epoch in range(num_epochs):
    for images, captions in train_loader:
        images = images.to(device)
        captions = captions.to(device)

        # Forward pass
        features = resnet(images)
        output = model(features, captions[:, :-1])

        # Compute loss
        output = output.view(-1, vocab_size)
        captions = captions[:, 1:].reshape(-1)
        loss = criterion(output, captions)

        # Backward pass and optimization
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

        # Print loss every 100 iterations
        if (i+1) % 100 == 0:
            print(f'Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{total_steps}], Loss: {loss.item():.4f}')

# Generate captions for test images
def generate_caption(image):
    image = transform(image).unsqueeze(0).to(device)
    feature = resnet(image).unsqueeze(1)
    caption_ids = [vocab['<START>']]
    for _ in range(max_length):
        caption_tensor = torch.LongTensor(caption_ids).unsqueeze(0).to(device)
        output = model(feature, caption_tensor)
        predicted_id = output.argmax(2)[:,-1].item()
        caption_ids.append(predicted_id)
        if predicted_id == vocab['<END>']:
            break
    return ' '.join([idx_to_word[idx] for idx in caption_ids[1:-1]])

# Example usage
image = Image.open('test_image.jpg')
caption = generate_caption(image)
print('Generated Caption:', caption)
